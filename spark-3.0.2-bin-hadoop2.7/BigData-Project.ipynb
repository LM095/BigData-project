{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>mid</th><th>title</th><th>year</th><th>rating</th><th>num_ratings</th></tr>\n",
       "<tr><td>1</td><td>Toy story</td><td>1995</td><td>3.7</td><td>102338</td></tr>\n",
       "<tr><td>2</td><td>Jumanji</td><td>1995</td><td>3.2</td><td>44587</td></tr>\n",
       "<tr><td>3</td><td>Grumpy Old Men</td><td>1993</td><td>3.2</td><td>10489</td></tr>\n",
       "<tr><td>4</td><td>Waiting to Exhale</td><td>1995</td><td>3.3</td><td>5666</td></tr>\n",
       "<tr><td>5</td><td>Father of the Bri...</td><td>1995</td><td>3</td><td>13761</td></tr>\n",
       "<tr><td>6</td><td>Heat</td><td>1995</td><td>3.9</td><td>42785</td></tr>\n",
       "<tr><td>7</td><td>Sabrina</td><td>1954</td><td>3.8</td><td>12812</td></tr>\n",
       "<tr><td>8</td><td>Tom and Huck</td><td>1995</td><td>2.7</td><td>2649</td></tr>\n",
       "<tr><td>9</td><td>Sudden Death</td><td>1995</td><td>2.6</td><td>3626</td></tr>\n",
       "<tr><td>10</td><td>GoldenEye</td><td>1995</td><td>3.4</td><td>28260</td></tr>\n",
       "<tr><td>11</td><td>The American Pres...</td><td>1995</td><td>3.2</td><td>8320</td></tr>\n",
       "<tr><td>12</td><td>Dracula: Dead and...</td><td>1995</td><td>2.8</td><td>10078</td></tr>\n",
       "<tr><td>13</td><td>Balto</td><td>1995</td><td>3.2</td><td>9195</td></tr>\n",
       "<tr><td>14</td><td>Nixon</td><td>1995</td><td>3.5</td><td>3256</td></tr>\n",
       "<tr><td>15</td><td>Cutthroat Island</td><td>1995</td><td>2.6</td><td>3350</td></tr>\n",
       "<tr><td>16</td><td>Casino</td><td>1995</td><td>3.9</td><td>66463</td></tr>\n",
       "<tr><td>17</td><td>Sense and Sensibi...</td><td>1995</td><td>3.8</td><td>32782</td></tr>\n",
       "<tr><td>18</td><td>Four Rooms</td><td>1995</td><td>3.5</td><td>14266</td></tr>\n",
       "<tr><td>19</td><td>Ace Ventura: When...</td><td>1995</td><td>3.2</td><td>87306</td></tr>\n",
       "<tr><td>20</td><td>Money Train</td><td>1995</td><td>2.7</td><td>5263</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+--------------------+----+------+-----------+\n",
       "|mid|               title|year|rating|num_ratings|\n",
       "+---+--------------------+----+------+-----------+\n",
       "|  1|           Toy story|1995|   3.7|     102338|\n",
       "|  2|             Jumanji|1995|   3.2|      44587|\n",
       "|  3|      Grumpy Old Men|1993|   3.2|      10489|\n",
       "|  4|   Waiting to Exhale|1995|   3.3|       5666|\n",
       "|  5|Father of the Bri...|1995|     3|      13761|\n",
       "|  6|                Heat|1995|   3.9|      42785|\n",
       "|  7|             Sabrina|1954|   3.8|      12812|\n",
       "|  8|        Tom and Huck|1995|   2.7|       2649|\n",
       "|  9|        Sudden Death|1995|   2.6|       3626|\n",
       "| 10|           GoldenEye|1995|   3.4|      28260|\n",
       "| 11|The American Pres...|1995|   3.2|       8320|\n",
       "| 12|Dracula: Dead and...|1995|   2.8|      10078|\n",
       "| 13|               Balto|1995|   3.2|       9195|\n",
       "| 14|               Nixon|1995|   3.5|       3256|\n",
       "| 15|    Cutthroat Island|1995|   2.6|       3350|\n",
       "| 16|              Casino|1995|   3.9|      66463|\n",
       "| 17|Sense and Sensibi...|1995|   3.8|      32782|\n",
       "| 18|          Four Rooms|1995|   3.5|      14266|\n",
       "| 19|Ace Ventura: When...|1995|   3.2|      87306|\n",
       "| 20|         Money Train|1995|   2.7|       5263|\n",
       "+---+--------------------+----+------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from pyspark.sql.functions import min, max, col, when, lit, udf\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "# load the data\n",
    "movies_file = sc.textFile(\"dataset/movies.csv\")\n",
    "genres_file = sc.textFile(\"dataset/genres.csv\")\n",
    "actors_file = sc.textFile(\"dataset/actors.csv\")\n",
    "tagNames_file = sc.textFile(\"dataset/tag_names.csv\")\n",
    "tags_file = sc.textFile(\"dataset/tags.csv\")\n",
    "\n",
    "# we separate the fields for each table in csv format\n",
    "data_movies = movies_file.map(lambda row: next(csv.reader(row.splitlines(), skipinitialspace=True)))\n",
    "data_genres = genres_file.map(lambda row: next(csv.reader(row.splitlines(), skipinitialspace=True)))\n",
    "data_actors = actors_file.map(lambda row: next(csv.reader(row.splitlines(), skipinitialspace=True)))\n",
    "data_tagNames = tagNames_file.map(lambda row: next(csv.reader(row.splitlines(), skipinitialspace=True)))\n",
    "data_tags = tags_file.map(lambda row: next(csv.reader(row.splitlines(), skipinitialspace=True)))\n",
    "\n",
    "# we create the dataframe for each data generated\n",
    "table_movies = spark.createDataFrame(data_movies, ['mid', 'title','year','rating','num_ratings'])\n",
    "table_genres = spark.createDataFrame(data_genres, ['mid', 'genre'])\n",
    "table_actors = spark.createDataFrame(data_actors, ['mid', 'name', 'cast_position'])\n",
    "table_tagNames = spark.createDataFrame(data_tagNames, ['tid', 'tag'])\n",
    "table_tags = spark.createDataFrame(data_tags, ['mid', 'tid'])\n",
    "\n",
    "# create an alias for each table\n",
    "movies = table_movies.alias('movies')\n",
    "genres = table_genres.alias('genres')\n",
    "actors = table_actors.alias('actors')\n",
    "tagNames = table_tagNames.alias('tagNames')\n",
    "tags = table_tags.alias('tags')\n",
    "\n",
    "# we mantain the tables in cache\n",
    "\n",
    "genres.persist()\n",
    "actors.persist()\n",
    "tagNames.persist()\n",
    "tags.persist()\n",
    "movies.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Print all movie titles starring ‘Daniel Craig’, sorted in an ascending alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>title</th><th>name</th></tr>\n",
       "<tr><td>A Kid in King Art...</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Archangel</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Casino Royale</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Casino Royale</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Elizabeth</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Enduring Love</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Infamous</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Lara Croft: Tomb ...</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Layer Cake</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Munich</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Quantum of Solace</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Renaissance</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Road to Perdition</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Sorstalanság</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>Sylvia</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>The Golden Compass</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>The Invasion</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>The Jacket</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>The Mother</td><td>Daniel Craig</td></tr>\n",
       "<tr><td>The Mother</td><td>Daniel Craig</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+------------+\n",
       "|               title|        name|\n",
       "+--------------------+------------+\n",
       "|A Kid in King Art...|Daniel Craig|\n",
       "|           Archangel|Daniel Craig|\n",
       "|       Casino Royale|Daniel Craig|\n",
       "|       Casino Royale|Daniel Craig|\n",
       "|           Elizabeth|Daniel Craig|\n",
       "|       Enduring Love|Daniel Craig|\n",
       "|            Infamous|Daniel Craig|\n",
       "|Lara Croft: Tomb ...|Daniel Craig|\n",
       "|          Layer Cake|Daniel Craig|\n",
       "|              Munich|Daniel Craig|\n",
       "|   Quantum of Solace|Daniel Craig|\n",
       "|         Renaissance|Daniel Craig|\n",
       "|   Road to Perdition|Daniel Craig|\n",
       "|        Sorstalanság|Daniel Craig|\n",
       "|              Sylvia|Daniel Craig|\n",
       "|  The Golden Compass|Daniel Craig|\n",
       "|        The Invasion|Daniel Craig|\n",
       "|          The Jacket|Daniel Craig|\n",
       "|          The Mother|Daniel Craig|\n",
       "|          The Mother|Daniel Craig|\n",
       "+--------------------+------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_with_actors = movies.join(actors, movies.mid == actors.mid)\n",
    "\n",
    "movies_title_actors = movies_with_actors.select('title','name')\n",
    "movies_with_Craig = movies_title_actors.filter(movies_title_actors.name == 'Daniel Craig').sort(movies_title_actors.title.asc())\n",
    "\n",
    "# display the result: we use count to display all the rows\n",
    "#movies_with_Craig.show(movies_with_Craig.count(), truncate=False)\n",
    "movies_with_Craig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Print names of the cast of the movie ‘The Dark Knight’ in an ascending alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th></tr>\n",
       "<tr><td>Aaron Eckhart</td></tr>\n",
       "<tr><td>Adam Kalesperis</td></tr>\n",
       "<tr><td>Aidan Feore</td></tr>\n",
       "<tr><td>Andrew Bicknell</td></tr>\n",
       "<tr><td>Andy Luther</td></tr>\n",
       "<tr><td>Anthony Michael Hall</td></tr>\n",
       "<tr><td>Ariyon Bakare</td></tr>\n",
       "<tr><td>Beatrice Rosen</td></tr>\n",
       "<tr><td>Bill Smille</td></tr>\n",
       "<tr><td>Brandon Lambdin</td></tr>\n",
       "<tr><td>Bronson Webb</td></tr>\n",
       "<tr><td>Chin Han</td></tr>\n",
       "<tr><td>Christian Bale</td></tr>\n",
       "<tr><td>Chucky Venn</td></tr>\n",
       "<tr><td>Cillian Murphy</td></tr>\n",
       "<tr><td>Colin McFarlane</td></tr>\n",
       "<tr><td>Craig Heaney</td></tr>\n",
       "<tr><td>Crhis Perschler</td></tr>\n",
       "<tr><td>Dale RIvera</td></tr>\n",
       "<tr><td>Daryl Satcher</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+\n",
       "|                name|\n",
       "+--------------------+\n",
       "|       Aaron Eckhart|\n",
       "|     Adam Kalesperis|\n",
       "|         Aidan Feore|\n",
       "|     Andrew Bicknell|\n",
       "|         Andy Luther|\n",
       "|Anthony Michael Hall|\n",
       "|       Ariyon Bakare|\n",
       "|      Beatrice Rosen|\n",
       "|         Bill Smille|\n",
       "|     Brandon Lambdin|\n",
       "|        Bronson Webb|\n",
       "|            Chin Han|\n",
       "|      Christian Bale|\n",
       "|         Chucky Venn|\n",
       "|      Cillian Murphy|\n",
       "|     Colin McFarlane|\n",
       "|        Craig Heaney|\n",
       "|     Crhis Perschler|\n",
       "|         Dale RIvera|\n",
       "|       Daryl Satcher|\n",
       "+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_with_actors = movies.join(actors, movies.mid == actors.mid)\n",
    "movies_title_actors = movies_with_actors.select('title','name')\n",
    "\n",
    "cast_TheDarkKnight = movies_title_actors.filter(movies_title_actors.title == 'The Dark Knight').sort(movies_title_actors.name.asc())\n",
    "only_cast = cast_TheDarkKnight.select('name')\n",
    "#only_cast.show(only_cast.count(), truncate=False)\n",
    "only_cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Print the distinct genres in the database and their corresponding number of movies N where N is greater than 1000, sorted in the ascending order of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>genre</th><th>countMovies</th></tr>\n",
       "<tr><td>Adventure</td><td>1003</td></tr>\n",
       "<tr><td>Crime</td><td>1086</td></tr>\n",
       "<tr><td>Action</td><td>1445</td></tr>\n",
       "<tr><td>Romance</td><td>1644</td></tr>\n",
       "<tr><td>Thriller</td><td>1664</td></tr>\n",
       "<tr><td>Comedy</td><td>3566</td></tr>\n",
       "<tr><td>Drama</td><td>5076</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+-----------+\n",
       "|    genre|countMovies|\n",
       "+---------+-----------+\n",
       "|Adventure|       1003|\n",
       "|    Crime|       1086|\n",
       "|   Action|       1445|\n",
       "|  Romance|       1644|\n",
       "| Thriller|       1664|\n",
       "|   Comedy|       3566|\n",
       "|    Drama|       5076|\n",
       "+---------+-----------+"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_genres = genres.rdd.map(lambda x: (x.genre,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: (int(x[1]) > 1000))\n",
    "#reduced_genres.takeOrdered(20, key = lambda x: -x[1])\n",
    "reduced_genresdf = reduced_genres.toDF(['genre', 'countMovies']).sort('countMovies')\n",
    "reduced_genresdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. For each year, print the movie title, year, and rating, sorted in the ascending order of year and the descending order of movie rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>title</th><th>year</th><th>rating</th></tr>\n",
       "<tr><td>The Great Train R...</td><td>1903</td><td>0</td></tr>\n",
       "<tr><td>The Birth of a Na...</td><td>1915</td><td>3.3</td></tr>\n",
       "<tr><td>Intolerance: Love...</td><td>1916</td><td>3.8</td></tr>\n",
       "<tr><td>The Immigrant</td><td>1917</td><td>0</td></tr>\n",
       "<tr><td>Otets Sergiy</td><td>1917</td><td>0</td></tr>\n",
       "<tr><td>A Dog&#x27;s Life</td><td>1918</td><td>0</td></tr>\n",
       "<tr><td>Broken Blossoms o...</td><td>1919</td><td>3.7</td></tr>\n",
       "<tr><td>Die Spinnen, 1. T...</td><td>1919</td><td>0</td></tr>\n",
       "<tr><td>Male and Female</td><td>1919</td><td>0</td></tr>\n",
       "<tr><td>Das Cabinet des D...</td><td>1920</td><td>4.1</td></tr>\n",
       "<tr><td>Der Golem, wie er...</td><td>1920</td><td>0</td></tr>\n",
       "<tr><td>The Saphead</td><td>1920</td><td>0</td></tr>\n",
       "<tr><td>Way Down East</td><td>1920</td><td>0</td></tr>\n",
       "<tr><td>Orphans of the Storm</td><td>1921</td><td>0</td></tr>\n",
       "<tr><td>The Goat</td><td>1921</td><td>0</td></tr>\n",
       "<tr><td>Dr. Mabuse, der S...</td><td>1922</td><td>4.1</td></tr>\n",
       "<tr><td>Dr. Mabuse, der S...</td><td>1922</td><td>4.1</td></tr>\n",
       "<tr><td>Nosferatu, eine S...</td><td>1922</td><td>3.9</td></tr>\n",
       "<tr><td>Häxan</td><td>1922</td><td>3.8</td></tr>\n",
       "<tr><td>Nanook of the North</td><td>1922</td><td>3.7</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+----+------+\n",
       "|               title|year|rating|\n",
       "+--------------------+----+------+\n",
       "|The Great Train R...|1903|     0|\n",
       "|The Birth of a Na...|1915|   3.3|\n",
       "|Intolerance: Love...|1916|   3.8|\n",
       "|       The Immigrant|1917|     0|\n",
       "|        Otets Sergiy|1917|     0|\n",
       "|        A Dog's Life|1918|     0|\n",
       "|Broken Blossoms o...|1919|   3.7|\n",
       "|Die Spinnen, 1. T...|1919|     0|\n",
       "|     Male and Female|1919|     0|\n",
       "|Das Cabinet des D...|1920|   4.1|\n",
       "|       Way Down East|1920|     0|\n",
       "|         The Saphead|1920|     0|\n",
       "|Der Golem, wie er...|1920|     0|\n",
       "|            The Goat|1921|     0|\n",
       "|Orphans of the Storm|1921|     0|\n",
       "|Dr. Mabuse, der S...|1922|   4.1|\n",
       "|Dr. Mabuse, der S...|1922|   4.1|\n",
       "|Nosferatu, eine S...|1922|   3.9|\n",
       "|               Häxan|1922|   3.8|\n",
       "| Nanook of the North|1922|   3.7|\n",
       "+--------------------+----+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sorted= movies.sort(movies.year.asc(), movies.rating.desc())\n",
    "movies_sortedFiltered = movies_sorted.select('title', 'year', 'rating')\n",
    "movies_sortedFiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Critiques say that some words used in tags to convey emotions are very recurrent. To convey positive and negative emotions, the words ‘good’ and ‘bad’, respectively, are used predominantly in tags. Print all movie titles whose audience opinion is split (i.e., has at least one audience who expresses positive emotion and at least one who expresses negative emotion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>title</th></tr>\n",
       "<tr><td>Twilight</td></tr>\n",
       "<tr><td>The Forgotten</td></tr>\n",
       "<tr><td>Starship Troopers</td></tr>\n",
       "<tr><td>Bridget Jones&#x27;s D...</td></tr>\n",
       "<tr><td>Howard the Duck</td></tr>\n",
       "<tr><td>Hercules in New York</td></tr>\n",
       "<tr><td>C.H.U.D.</td></tr>\n",
       "<tr><td>The Wicker Man</td></tr>\n",
       "<tr><td>Ocean&#x27;s Eleven</td></tr>\n",
       "<tr><td>Return of the Kil...</td></tr>\n",
       "<tr><td>From Dusk Till Dawn</td></tr>\n",
       "<tr><td>Kakushi-toride no...</td></tr>\n",
       "<tr><td>Xanadu</td></tr>\n",
       "<tr><td>Lawrence of Arabia</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+\n",
       "|               title|\n",
       "+--------------------+\n",
       "|            Twilight|\n",
       "|       The Forgotten|\n",
       "|   Starship Troopers|\n",
       "|Bridget Jones's D...|\n",
       "|     Howard the Duck|\n",
       "|Hercules in New York|\n",
       "|            C.H.U.D.|\n",
       "|      The Wicker Man|\n",
       "|      Ocean's Eleven|\n",
       "|Return of the Kil...|\n",
       "| From Dusk Till Dawn|\n",
       "|Kakushi-toride no...|\n",
       "|              Xanadu|\n",
       "|  Lawrence of Arabia|\n",
       "+--------------------+"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_with_tags = movies.join(tags, ['mid']).join(tagNames, ['tid'])\n",
    "movies_with_tags_onlyGood = movies_with_tags.filter(movies_with_tags.tag.contains( 'good'))\n",
    "movies_with_tags_onlyBad = movies_with_tags.filter(movies_with_tags.tag.contains( 'bad'))\n",
    "\n",
    "movies_with_tags_onlyGoodBad = (movies_with_tags_onlyGood.select('mid','title')).intersect(movies_with_tags_onlyBad.select('mid','title'))\n",
    "result = movies_with_tags_onlyGoodBad.select('title')\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. One would expect that the movie with the highest number of user ratings is either the highest rated movie or perhaps the lowest rated movie. Let’s find out if this is the case here:\n",
    "\n",
    "#### 6.1 : Print all information (mid, title, year, num ratings, rating) for the movie(s) with the highest number of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>mid</th><th>title</th><th>year</th><th>rating</th><th>num_ratings</th></tr>\n",
       "<tr><td>4201</td><td>Pirates of the Ca...</td><td>2007</td><td>3.8</td><td>1768593</td></tr>\n",
       "<tr><td>53125</td><td>Pirates of the Ca...</td><td>2007</td><td>3.8</td><td>1768593</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+--------------------+----+------+-----------+\n",
       "|  mid|               title|year|rating|num_ratings|\n",
       "+-----+--------------------+----+------+-----------+\n",
       "| 4201|Pirates of the Ca...|2007|   3.8|    1768593|\n",
       "|53125|Pirates of the Ca...|2007|   3.8|    1768593|\n",
       "+-----+--------------------+----+------+-----------+"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ratings = movies.filter(movies.num_ratings != '\\\\N').withColumn('num_ratings', col('num_ratings').cast('int'))\n",
    "max_num_rating = num_ratings.select(max('num_ratings'))\n",
    "\n",
    "movies_with_max_num_ratings = movies.filter(movies.num_ratings == max_num_rating.collect()[0][0])\n",
    "movies_with_max_num_ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2: Print all information (mid, title, year, num ratings, rating) for the movie(s) with the highest rating (include tuples that tie), sorted by the ascending order of movie id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>mid</th><th>title</th><th>year</th><th>rating</th><th>num_ratings</th></tr>\n",
       "<tr><td>4311</td><td>1732 Høtten</td><td>1998</td><td>5</td><td>5</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-----------+----+------+-----------+\n",
       "| mid|      title|year|rating|num_ratings|\n",
       "+----+-----------+----+------+-----------+\n",
       "|4311|1732 Høtten|1998|     5|          5|\n",
       "+----+-----------+----+------+-----------+"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ratings = movies.filter(movies.rating != '\\\\N').withColumn('rating', col('rating').cast('double'))\n",
    "max_rating = num_ratings.select(max('rating'))\n",
    "movies_with_max_ratings = movies.filter(movies.rating == max_rating.collect()[0][0])\n",
    "movies_with_max_ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3: Is (Are) the movie(s) with the most number of user ratings among these highest rated movies? Print the output of the query that will check our conjecture (i.e., your query will print the movie(s) that has (have) the highest number of ratings as well as the highest rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>mid</th><th>title</th><th>year</th><th>rating</th><th>num_ratings</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+-----+----+------+-----------+\n",
       "|mid|title|year|rating|num_ratings|\n",
       "+---+-----+----+------+-----------+\n",
       "+---+-----+----+------+-----------+"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = movies_with_max_num_ratings.intersect(movies_with_max_ratings)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4: Print all information (mid, title, year, num ratings, rating) for the movie(s) with the lowest rating (include tuples that tie), sorted by the ascending order of movie id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>mid</th><th>title</th><th>year</th><th>rating</th><th>num_ratings</th></tr>\n",
       "<tr><td>32</td><td>Twelve Monkeys</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>33</td><td>Wings of Courage</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>39</td><td>Clueless</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>56</td><td>Kids of the Round...</td><td>1997</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>59</td><td>Le confessionnal</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>61</td><td>Eye for an Eye</td><td>1996</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>63</td><td>Don&#x27;t Be a Menace...</td><td>1996</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>69</td><td>Friday</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>115</td><td>Le bonheur est da...</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>126</td><td>The Neverending S...</td><td>1994</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>128</td><td>Jupiter&#x27;s Wife</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>129</td><td>Pie in the Sky</td><td>1996</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>134</td><td>Sonic Outlaws</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>139</td><td>Hard Target</td><td>1993</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>178</td><td>Love &amp; Human Remains</td><td>1993</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>179</td><td>Mad Love</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>184</td><td>Nadja</td><td>1994</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>189</td><td>The Reckless Moment</td><td>1949</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>190</td><td>Fail-Safe</td><td>1964</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>220</td><td>Castle Freak</td><td>1995</td><td>0</td><td>0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+--------------------+----+------+-----------+\n",
       "|mid|               title|year|rating|num_ratings|\n",
       "+---+--------------------+----+------+-----------+\n",
       "| 32|      Twelve Monkeys|1995|     0|          0|\n",
       "| 33|    Wings of Courage|1995|     0|          0|\n",
       "| 39|            Clueless|1995|     0|          0|\n",
       "| 56|Kids of the Round...|1997|     0|          0|\n",
       "| 59|    Le confessionnal|1995|     0|          0|\n",
       "| 61|      Eye for an Eye|1996|     0|          0|\n",
       "| 63|Don't Be a Menace...|1996|     0|          0|\n",
       "| 69|              Friday|1995|     0|          0|\n",
       "|115|Le bonheur est da...|1995|     0|          0|\n",
       "|126|The Neverending S...|1994|     0|          0|\n",
       "|128|      Jupiter's Wife|1995|     0|          0|\n",
       "|129|      Pie in the Sky|1996|     0|          0|\n",
       "|134|       Sonic Outlaws|1995|     0|          0|\n",
       "|139|         Hard Target|1993|     0|          0|\n",
       "|178|Love & Human Remains|1993|     0|          0|\n",
       "|179|            Mad Love|1995|     0|          0|\n",
       "|184|               Nadja|1994|     0|          0|\n",
       "|189| The Reckless Moment|1949|     0|          0|\n",
       "|190|           Fail-Safe|1964|     0|          0|\n",
       "|220|        Castle Freak|1995|     0|          0|\n",
       "+---+--------------------+----+------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ratings = movies.filter(movies.rating != '\\\\N').withColumn('rating', col('rating').cast('double'))\n",
    "min_rating = num_ratings.select(min('rating'))\n",
    "movies_with_min_ratings = movies.filter(movies.rating == min_rating.collect()[0][0])\n",
    "movies_with_min_ratings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5: Is (Are) the movie(s) with the most number of user ratings among these lowest rated movies? Print the output of the query that will check our conjecture (i.e., your query will print the movie(s) that has (have) the highest number of ratings as well as the lowest rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>mid</th><th>title</th><th>year</th><th>rating</th><th>num_ratings</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+-----+----+------+-----------+\n",
       "|mid|title|year|rating|num_ratings|\n",
       "+---+-----+----+------+-----------+\n",
       "+---+-----+----+------+-----------+"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = movies_with_max_num_ratings.intersect(movies_with_min_ratings)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6: In conclusion, is our hypothesis or conjecture true for the MovieLens database?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the results of query 6.3 and 6.5 show that both the intersection of max_num_ratings and max_rating/min_rating are empty! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Print the movie title, year, and rating of the lowest and highest movies for each year in 2005 – 2011, inclusive, in the ascending order of year. In case of a tie, print the records in the ascending order of title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>title</th><th>year</th><th>rating</th></tr>\n",
       "<tr><td>Alone in the Dark</td><td>2005</td><td>2.1</td></tr>\n",
       "<tr><td>Alone in the Dark</td><td>2005</td><td>2.1</td></tr>\n",
       "<tr><td>Alone in the Dark</td><td>2005</td><td>2.1</td></tr>\n",
       "<tr><td>Son of the Mask</td><td>2005</td><td>2.1</td></tr>\n",
       "<tr><td>No Direction Home...</td><td>2005</td><td>4.3</td></tr>\n",
       "<tr><td>Basic Instinct 2</td><td>2006</td><td>2.5</td></tr>\n",
       "<tr><td>Basic Instinct 2</td><td>2006</td><td>2.5</td></tr>\n",
       "<tr><td>Bug</td><td>2006</td><td>2.5</td></tr>\n",
       "<tr><td>Bug</td><td>2006</td><td>2.5</td></tr>\n",
       "<tr><td>Doogal</td><td>2006</td><td>2.5</td></tr>\n",
       "<tr><td>Das Leben der And...</td><td>2006</td><td>4.4</td></tr>\n",
       "<tr><td>D-War</td><td>2007</td><td>2.3</td></tr>\n",
       "<tr><td>Byôsoku 5 senchim...</td><td>2007</td><td>4.3</td></tr>\n",
       "<tr><td>No End in Sight</td><td>2007</td><td>4.3</td></tr>\n",
       "<tr><td>Pete Seeger: The ...</td><td>2007</td><td>4.3</td></tr>\n",
       "<tr><td>War Dance</td><td>2007</td><td>4.3</td></tr>\n",
       "<tr><td>Asylum</td><td>2008</td><td>2.2</td></tr>\n",
       "<tr><td>Asylum</td><td>2008</td><td>2.2</td></tr>\n",
       "<tr><td>The Dark Knight</td><td>2008</td><td>4.5</td></tr>\n",
       "<tr><td>Miss March</td><td>2009</td><td>2.7</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+----+------+\n",
       "|               title|year|rating|\n",
       "+--------------------+----+------+\n",
       "|   Alone in the Dark|2005|   2.1|\n",
       "|   Alone in the Dark|2005|   2.1|\n",
       "|   Alone in the Dark|2005|   2.1|\n",
       "|     Son of the Mask|2005|   2.1|\n",
       "|No Direction Home...|2005|   4.3|\n",
       "|    Basic Instinct 2|2006|   2.5|\n",
       "|    Basic Instinct 2|2006|   2.5|\n",
       "|                 Bug|2006|   2.5|\n",
       "|                 Bug|2006|   2.5|\n",
       "|              Doogal|2006|   2.5|\n",
       "|Das Leben der And...|2006|   4.4|\n",
       "|               D-War|2007|   2.3|\n",
       "|Byôsoku 5 senchim...|2007|   4.3|\n",
       "|     No End in Sight|2007|   4.3|\n",
       "|Pete Seeger: The ...|2007|   4.3|\n",
       "|           War Dance|2007|   4.3|\n",
       "|              Asylum|2008|   2.2|\n",
       "|              Asylum|2008|   2.2|\n",
       "|     The Dark Knight|2008|   4.5|\n",
       "|          Miss March|2009|   2.7|\n",
       "+--------------------+----+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_2005_2011 = movies.filter((movies.year >= 2005) & (movies.year <= 2011))\n",
    "movies_list = []\n",
    "\n",
    "for year in range(2005,2012):\n",
    "    ratings = movies_2005_2011.filter((movies_2005_2011.year == year) & (movies_2005_2011.num_ratings != '\\\\N') & (movies_2005_2011.num_ratings > 0)).withColumn('rating', col('rating').cast('double'))\n",
    "    min_rating = ratings.select(min('rating'))\n",
    "    max_rating = ratings.select(max('rating'))\n",
    "    result_min = ratings.filter(ratings.rating == min_rating.collect()[0][0]).sort(ratings.title)\n",
    "    result_max = ratings.filter(ratings.rating == max_rating.collect()[0][0]).sort(ratings.title)\n",
    "    final_result = result_min.union(result_max).select('title','year', 'rating')\n",
    "    movies_list.extend(final_result.collect())\n",
    "\n",
    "rdd = sc.parallelize(movies_list)\n",
    "result = rdd.toDF()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Let us find out who are the ‘no flop’ actors. A ‘no flop’ actor can be defined as one who has played only in movies which have a rating greater than or equal to 4. We split this problem into the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.1 Create a view called high ratings which contains the distinct names of all actors who have played in movies with a rating greater than or equal to 4. Similarly, create a view called low ratings which contains the distinct names of all actors who have played in movies with a rating less than 4. Print the number of rows in each view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87032"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_with_actors = movies.join(actors,['mid']).withColumn('rating', col('rating').cast('double'))\n",
    "\n",
    "high_ratings = movies_with_actors.filter(movies_with_actors.rating >= 4.0).select('name').distinct()\n",
    "low_ratings = movies_with_actors.filter(movies_with_actors.rating < 4.0).select('name').distinct()\n",
    "high_ratings.count()\n",
    "low_ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.2 Use the above views to print the number of ‘no flop’ actors in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7015"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = high_ratings.subtract(low_ratings)\n",
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8.3 For each ‘no flop’ actor, print the name of the actor and the number of movies N that he/she played in, sorted in descending order of N. Finally, print the top10 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>countMovies</th></tr>\n",
       "<tr><td>Nikolai Grinko</td><td>8</td></tr>\n",
       "<tr><td>Paul Frankeur</td><td>7</td></tr>\n",
       "<tr><td>John Cazale</td><td>7</td></tr>\n",
       "<tr><td>Tsutomu Yamazaki</td><td>6</td></tr>\n",
       "<tr><td>Gunnel Lindblom</td><td>6</td></tr>\n",
       "<tr><td>Allan Garcia</td><td>6</td></tr>\n",
       "<tr><td>Kuniko Miyake</td><td>6</td></tr>\n",
       "<tr><td>Anatoli Solonitsin</td><td>5</td></tr>\n",
       "<tr><td>Timothy T. Mitchum</td><td>5</td></tr>\n",
       "<tr><td>Megan Gallagher</td><td>5</td></tr>\n",
       "<tr><td>Michael Drayer</td><td>4</td></tr>\n",
       "<tr><td>Bilal Bishop</td><td>4</td></tr>\n",
       "<tr><td>Yvette Bodrick</td><td>4</td></tr>\n",
       "<tr><td>Travis Veada</td><td>4</td></tr>\n",
       "<tr><td>Michael Nyqvist</td><td>4</td></tr>\n",
       "<tr><td>Ellen Bahl</td><td>4</td></tr>\n",
       "<tr><td>Jindai Joseph</td><td>4</td></tr>\n",
       "<tr><td>Jamia Simone Nash</td><td>4</td></tr>\n",
       "<tr><td>Dirk Zeelenberg</td><td>4</td></tr>\n",
       "<tr><td>Tyler McGuckin</td><td>4</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------------------+-----------+\n",
       "|               name|countMovies|\n",
       "+-------------------+-----------+\n",
       "|     Nikolai Grinko|          8|\n",
       "|      Paul Frankeur|          7|\n",
       "|        John Cazale|          7|\n",
       "|    Gunnel Lindblom|          6|\n",
       "|       Allan Garcia|          6|\n",
       "|      Kuniko Miyake|          6|\n",
       "|   Tsutomu Yamazaki|          6|\n",
       "| Anatoli Solonitsin|          5|\n",
       "| Timothy T. Mitchum|          5|\n",
       "|    Megan Gallagher|          5|\n",
       "|     Tyler McGuckin|          4|\n",
       "|    Michael Nyqvist|          4|\n",
       "|     Yvette Bodrick|          4|\n",
       "|         Olaf Storm|          4|\n",
       "|Joyce Walker Jospeh|          4|\n",
       "|      Jindai Joseph|          4|\n",
       "|    Dirk Zeelenberg|          4|\n",
       "|     Michael Drayer|          4|\n",
       "|       Aaron Staton|          4|\n",
       "|  Jamia Simone Nash|          4|\n",
       "+-------------------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noFlopactors_with_mid = actors.join(result,['name'])\n",
    "\n",
    "reduced_actors = noFlopactors_with_mid.rdd.map(lambda x: (x.name,1)).reduceByKey(lambda a,b: a+b)\n",
    "reduced_actors_df = reduced_actors.toDF(['name', 'countMovies'])\n",
    "final_result = reduced_actors_df.withColumn('countMovies', col('countMovies').cast('int')).sort(reduced_actors_df.countMovies.desc())\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Let us find out who is the actor with the highest ‘longevity.’ Print the name of the actor/actress who has been playing in movies for the longest period of time (i.e., the time interval between their first movie and their last movie is the greatest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>difference</th></tr>\n",
       "<tr><td>Morgan Jones</td><td>102</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+----------+\n",
       "|        name|difference|\n",
       "+------------+----------+\n",
       "|Morgan Jones|       102|\n",
       "+------------+----------+"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_with_actors = movies.join(actors,['mid']).withColumn('year', col('year').cast('int'))\n",
    "partial_result = movies_with_actors.select('name','year').distinct()\n",
    "\n",
    "result_max = partial_result.groupBy('name').max('year').withColumnRenamed('max(year)', 'recentYear')\n",
    "result_min = partial_result.groupBy('name').min('year').withColumnRenamed('min(year)', 'firstYear')\n",
    "result_join = result_min.join(result_max,['name'])\n",
    "result = result_join.select('name',result_join.recentYear-result_join.firstYear).withColumnRenamed('(recentYear - firstYear)', 'difference')\n",
    "max_difference = result.select(max('difference'))\n",
    "final_result = result.filter(result.difference == max_difference.collect()[0][0])\n",
    "final_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Let us find the close friends of Annette Nicole. Print the names of all actors who have starred in (at least) all movies in which Annette Nicole has starred in. Note that it is OK if these actors have starred in more movies than Annette Nicole has played in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10.1 First, create a view called co_actors, which returns the distinct names of actors who played in at least one movie with Annette Nicole. Print the number of rows in this view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_with_actors = movies.join(actors,['mid'])\n",
    "movies_with_Annette = movies_with_actors.filter(movies_with_actors.name == 'Annette Nicole')\n",
    "movies_with_Annette_mid = movies_with_Annette.select('mid')\n",
    "movies_with_cast_and_Annette = actors.join(movies_with_Annette_mid,['mid'])\n",
    "co_actors = movies_with_cast_and_Annette.select('name').distinct()\n",
    "co_actors.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10.2 Second, create a view called all_combinations which returns all possible combinations of co_actors and the movie ids in which Annette Nicole played. Print the number of rows in this view. Note how that this view contains fake (co_actor, mid) combinations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_all_combinations = movies_with_Annette.withColumnRenamed('name', 'nameAnnette').crossJoin(co_actors)\n",
    "all_combinations = partial_all_combinations.select('mid','name')\n",
    "all_combinations.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10.3 Third, create a view called non_existent from the view all_combinations by removing all legitimate (co_actor,mid) pairs (i.e., pairs that exist in the actors table). Print the number of rows in this view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actors_real = actors.select('mid', 'name')\n",
    "\n",
    "non_existent = all_combinations.subtract(actors_real)\n",
    "#non_existent.select('name').distinct().count()\n",
    "non_existent.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10.4 Finally, from the view co_actors, eliminate the distinct actors that appear in the view non_extistent. Print the names of all co_actors except Annette Nicole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th></tr>\n",
       "<tr><td>Kristen Connolly</td></tr>\n",
       "<tr><td>Christian Perry</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------+\n",
       "|            name|\n",
       "+----------------+\n",
       "|Kristen Connolly|\n",
       "| Christian Perry|\n",
       "+----------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = co_actors.subtract(non_existent.select('name'))\n",
    "final.filter(final.name != 'Annette Nicole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Let us find out who is the most social actor. A social actor is the one with the highest number of distinct co-actors. We will break this into two sub-tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11.1 For the actor Tom Cruise, print his name and the number of distinct co-actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>countCoActors</th></tr>\n",
       "<tr><td>Tom Cruise</td><td>1238</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+\n",
       "|      name|countCoActors|\n",
       "+----------+-------------+\n",
       "|Tom Cruise|         1238|\n",
       "+----------+-------------+"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_with_Tom = actors.filter(actors.name == 'Tom Cruise').select('mid','name') \n",
    "movies_with_coActors = movies_with_Tom.join(actors.withColumnRenamed('name', 'nameCoActors'),['mid'])\n",
    "final_coActors_Tom = movies_with_coActors.filter(movies_with_coActors.nameCoActors != 'Tom Cruise').select('name', 'nameCoActors').distinct().groupBy('name').count().withColumnRenamed('count', 'countCoActors')\n",
    "final_coActors_Tom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11.2 For each actor, compute the number of distinct co-actors. For the highest such number, print the name of the actor and the number of distinct co-actors. In case of a tie, print the records sorted in alphabetical order by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_1</th><th>_2</th></tr>\n",
       "<tr><td>Oliver Platt</td><td>831</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+---+\n",
       "|          _1| _2|\n",
       "+------------+---+\n",
       "|Oliver Platt|831|\n",
       "+------------+---+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_social_actor = []\n",
    "maxCoActors = 0\n",
    "actors_list = actors.select('name').distinct().collect()\n",
    "\n",
    "for actor in actors_list:\n",
    "    movies_with_specific_actor = actors.filter(actors.name == actor[0]).select('mid','name') \n",
    "    movies_with_coActors = movies_with_specific_actor.join(actors.withColumnRenamed('name', 'nameCoActors'),['mid'])\n",
    "    final_coActors_specific_actor = movies_with_coActors.filter(movies_with_coActors.nameCoActors != actor[0]).select('name', 'nameCoActors').distinct().groupBy('name').count().withColumnRenamed('count', 'countCoActors')\n",
    "    most_social_actor.append((actor[0], final_coActors_specific_actor.collect()[0][1]))\n",
    "    \n",
    "rdd = sc.parallelize(most_social_actor)\n",
    "result = rdd.toDF()\n",
    "max_coActors = result.select(max('_2'))\n",
    "\n",
    "final_result = result.filter(result._2 == max_coActors.collect()[0][0]) \n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>countCoActors</th></tr>\n",
       "<tr><td>Samuel L. Jackson</td><td>1824</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+\n",
       "|             name|countCoActors|\n",
       "+-----------------+-------------+\n",
       "|Samuel L. Jackson|         1824|\n",
       "+-----------------+-------------+"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_actors = actors.rdd.map(lambda x: (x.mid,1)).reduceByKey(lambda a,b: a+b)\n",
    "result_countActors = reduced_actors.toDF().withColumnRenamed('_1','mid').withColumnRenamed('_2','countActors')\n",
    "movies_with_countActors = result_countActors.join(actors,['mid']).select('mid','countActors','name')\n",
    "\n",
    "movies_with_countActors_reduced = movies_with_countActors.rdd.map(lambda x: (x.name,x.countActors-1)).reduceByKey(lambda a,b: a+b)\n",
    "result_countActors = movies_with_countActors_reduced.toDF().withColumnRenamed('_1','name').withColumnRenamed('_2','countCoActors')\n",
    "\n",
    "# search for duplicates, we tried use mapReduce but it was to slow. Surprisingly groupBy and cout() was faster than mapReduce.\n",
    "couples_coActors = actors.join(actors.withColumnRenamed('name','coActor'),['mid']).groupBy('name','coActor').count().withColumnRenamed('count', 'countCoActors')\n",
    "result_partial_couples = couples_coActors.filter(couples_coActors.name != couples_coActors.coActor).select('name', 'coActor',couples_coActors.countCoActors-1).withColumnRenamed('(countCoActors - 1)', 'duplicates').withColumn('duplicates', col('duplicates').cast('int'))\n",
    "result_couples = result_partial_couples.groupBy('name').sum('duplicates').withColumnRenamed('sum(duplicates)', 'duplicates')\n",
    "\n",
    "r= result_countActors.join(result_couples,['name'])\n",
    "final_r = r.select('name', r.countCoActors - r.duplicates).withColumnRenamed('(countCoActors - duplicates)','countCoActors')\n",
    "max_coActors = final_r.select(max('countCoActors'))\n",
    "#final_r.filter(final_r.name == 'Tom Cruise') --> 1238 it's correct!\n",
    "final_result = final_r.filter(final_r.countCoActors == max_coActors.collect()[0][0]) \n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. We will now write some queries for a Content-Based Movie Recommendation System such as NetFlix. However, in this project we shall deploy a simple algorithm that may or may not produce optimal recommendations. Content-based recommendations focus on the properties of items, in our case movies. The similarity of two movies is determined by measuring the similarity of their properties. For a movie item, we shall consider the following five properties: actors, tags, genres, year, and rating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Given two movies X and Y, the similarity of Y to X, sim(X,Y), can be computed as: (fraction of common actors + fraction of common tags + fraction of common genres + age gap + rating gap) /5 where fraction is the number of common elements between X and Y divided by the number of elements of X, age gap is the normalized difference between the production years of X and Y, and rating gap is the normalized difference between the ratings of X and Y. Intuitively, the smaller the gaps are, the better (since movies of the same decade and rating are more likely to be similar). Moreover, note that we divide by five because each property is given an equal weight of 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Given a user who is known to like the movie ‘Mr. & Mrs. Smith’, write a query that prints the movie title, rating, and similarity percentage (i.e., similarity * 100) for the top 10 movies that are most similar to the ‘Mr. & Mrs. Smith’ movie, ordered by the similarity percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(mid='1')\n",
      "Row(mid='2')\n",
      "Row(mid='3')\n",
      "Row(mid='4')\n",
      "Row(mid='5')\n",
      "Row(mid='6')\n",
      "Row(mid='7')\n",
      "Row(mid='8')\n",
      "Row(mid='9')\n",
      "Row(mid='10')\n",
      "Row(mid='11')\n",
      "Row(mid='12')\n",
      "Row(mid='13')\n",
      "Row(mid='14')\n",
      "Row(mid='15')\n",
      "Row(mid='16')\n",
      "Row(mid='17')\n",
      "Row(mid='18')\n",
      "Row(mid='19')\n",
      "Row(mid='20')\n",
      "Row(mid='21')\n",
      "Row(mid='22')\n",
      "Row(mid='23')\n",
      "Row(mid='24')\n",
      "Row(mid='25')\n",
      "Row(mid='26')\n",
      "Row(mid='27')\n",
      "Row(mid='28')\n",
      "Row(mid='29')\n",
      "Row(mid='30')\n",
      "Row(mid='31')\n",
      "Row(mid='32')\n",
      "Row(mid='33')\n",
      "Row(mid='34')\n",
      "Row(mid='35')\n",
      "Row(mid='36')\n",
      "Row(mid='37')\n",
      "Row(mid='38')\n",
      "Row(mid='39')\n",
      "Row(mid='40')\n",
      "Row(mid='41')\n",
      "Row(mid='42')\n",
      "Row(mid='43')\n",
      "Row(mid='44')\n",
      "Row(mid='45')\n",
      "Row(mid='46')\n",
      "Row(mid='47')\n",
      "Row(mid='48')\n",
      "Row(mid='49')\n",
      "Row(mid='50')\n",
      "Row(mid='52')\n",
      "Row(mid='53')\n",
      "Row(mid='54')\n",
      "Row(mid='55')\n",
      "Row(mid='56')\n",
      "Row(mid='57')\n",
      "Row(mid='58')\n",
      "Row(mid='59')\n",
      "Row(mid='60')\n",
      "Row(mid='61')\n",
      "Row(mid='62')\n",
      "Row(mid='63')\n",
      "Row(mid='64')\n",
      "Row(mid='65')\n",
      "Row(mid='66')\n",
      "Row(mid='67')\n",
      "Row(mid='68')\n",
      "Row(mid='69')\n",
      "Row(mid='70')\n",
      "Row(mid='71')\n",
      "Row(mid='72')\n",
      "Row(mid='73')\n",
      "Row(mid='74')\n",
      "Row(mid='75')\n",
      "Row(mid='76')\n",
      "Row(mid='77')\n",
      "Row(mid='78')\n",
      "Row(mid='79')\n",
      "Row(mid='80')\n",
      "Row(mid='81')\n",
      "Row(mid='82')\n",
      "Row(mid='83')\n",
      "Row(mid='84')\n",
      "Row(mid='85')\n",
      "Row(mid='86')\n",
      "Row(mid='87')\n",
      "Row(mid='88')\n",
      "Row(mid='89')\n",
      "Row(mid='90')\n",
      "Row(mid='92')\n",
      "Row(mid='93')\n",
      "Row(mid='94')\n",
      "Row(mid='95')\n",
      "Row(mid='96')\n",
      "Row(mid='97')\n",
      "Row(mid='98')\n",
      "Row(mid='99')\n",
      "Row(mid='100')\n",
      "Row(mid='101')\n",
      "Row(mid='102')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>title</th><th>rating</th><th>similarity</th></tr>\n",
       "<tr><td>Waiting to Exhale</td><td>3.3</td><td>61.0</td></tr>\n",
       "<tr><td>Mighty Aphrodite</td><td>3.3</td><td>60.0</td></tr>\n",
       "<tr><td>Gazon maudit</td><td>3.4</td><td>60.0</td></tr>\n",
       "<tr><td>The American Pres...</td><td>3.2</td><td>59.0</td></tr>\n",
       "<tr><td>Grumpy Old Men</td><td>3.2</td><td>59.0</td></tr>\n",
       "<tr><td>Beautiful Girls</td><td>3.6</td><td>59.0</td></tr>\n",
       "<tr><td>Bottle Rocket</td><td>3.7</td><td>59.0</td></tr>\n",
       "<tr><td>Sabrina</td><td>3.8</td><td>57.99999999999999</td></tr>\n",
       "<tr><td>Sense and Sensibi...</td><td>3.8</td><td>57.99999999999999</td></tr>\n",
       "<tr><td>Vampire in Brooklyn</td><td>2.4</td><td>57.99999999999999</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------+-----------------+\n",
       "|               title|rating|       similarity|\n",
       "+--------------------+------+-----------------+\n",
       "|   Waiting to Exhale|   3.3|             61.0|\n",
       "|    Mighty Aphrodite|   3.3|             60.0|\n",
       "|        Gazon maudit|   3.4|             60.0|\n",
       "|The American Pres...|   3.2|             59.0|\n",
       "|      Grumpy Old Men|   3.2|             59.0|\n",
       "|     Beautiful Girls|   3.6|             59.0|\n",
       "|       Bottle Rocket|   3.7|             59.0|\n",
       "|             Sabrina|   3.8|57.99999999999999|\n",
       "|Sense and Sensibi...|   3.8|57.99999999999999|\n",
       "| Vampire in Brooklyn|   2.4|57.99999999999999|\n",
       "+--------------------+------+-----------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minYear = int(movies.select(min(movies.year)).collect()[0][0])\n",
    "maxYear = int(movies.select(max(movies.year)).collect()[0][0])\n",
    "maxRating = float(movies.filter(movies.rating != '\\\\N').select(max(movies.rating)).collect()[0][0])\n",
    "\n",
    "def get_mid(movie):\n",
    "    return movies.filter(movies.title == movie).select('mid').collect()[0][0]\n",
    "\n",
    "\n",
    "def similarity(mid_movie1, mid_movie2):\n",
    "    \n",
    "    get_num_actor = len(actors.filter(actors.mid == mid_movie1).select('name').collect())\n",
    "    get_num_tags = len(tags.filter(tags.mid == mid_movie1).select('tid').distinct().collect())\n",
    "    get_num_genres = len(genres.filter(genres.mid == mid_movie1).select('genre').distinct().collect())\n",
    "    \n",
    "    actorsMovie1 = actors.filter(actors.mid == mid_movie1).select('name')\n",
    "    actorsMovie2 = actors.filter(actors.mid == mid_movie2).select('name')\n",
    "    intersect_actor =  len(actorsMovie1.intersect(actorsMovie2).collect())/get_num_actor\n",
    "    \n",
    "    tagsMovie1 = tags.filter(tags.mid == mid_movie1).select('tid')\n",
    "    tagsMovie2 = tags.filter(tags.mid == mid_movie2).select('tid')\n",
    "    intersect_tag = len(tagsMovie1.intersect(tagsMovie2).collect())/get_num_tags\n",
    "    \n",
    "    genreMovie1 = genres.filter(genres.mid == mid_movie1).select('genre')\n",
    "    genreMovie2 = genres.filter(genres.mid == mid_movie2).select('genre')\n",
    "    intersect_genre =  len(genreMovie1.intersect(genreMovie2).collect())/get_num_genres\n",
    "    \n",
    "    year_movie1 = int(movies.filter(movies.mid == mid_movie1).select('year').collect()[0][0])\n",
    "    year_movie2 = int(movies.filter(movies.mid == mid_movie2).select('year').collect()[0][0])\n",
    "    difference = abs(year_movie1 - year_movie2)\n",
    "    age_gap =  1 - (difference/maxYear)\n",
    "    \n",
    "    \n",
    "    rating_movie1 = float(movies.filter(movies.mid == mid_movie1).select('rating').collect()[0][0])\n",
    "    rating_movie2 = float(movies.filter(movies.mid == mid_movie2).select('rating').collect()[0][0])\n",
    "    difference_gap = abs(rating_movie1 - rating_movie2) \n",
    "    rating_gap = 1 - (difference_gap/maxRating)\n",
    "    similarity = round((intersect_actor+intersect_tag+intersect_genre + age_gap + rating_gap)/5,2)*100\n",
    "    \n",
    "    myFloatRdd = sc.parallelize([similarity])\n",
    "    row = Row(\"similarity\") \n",
    "    result = myFloatRdd.map(row).toDF()\n",
    "\n",
    "    \n",
    "    return similarity#result.select('similarity')\n",
    "\n",
    "\n",
    "mid_movieMrSmith = get_mid('Mr. & Mrs. Smith')\n",
    "movies_without_MrSmith = movies.filter(movies.mid != mid_movieMrSmith).filter(movies.rating != '\\\\N').limit(100).select('mid')\n",
    "similarity_for_movies = []\n",
    "#print(len(movies_without_MrSmith.collect()))\n",
    "\n",
    "for mid in movies_without_MrSmith.collect():\n",
    "    similarity_for_movies.append([mid_movieMrSmith,mid[0], similarity(mid_movieMrSmith,mid[0])])\n",
    "\n",
    "\n",
    "\n",
    "rdd = sc.parallelize(similarity_for_movies)\n",
    "result = rdd.toDF().withColumnRenamed('_1','mid_movieMrSmith').withColumnRenamed('_2','mid').withColumnRenamed('_3','similarity')\n",
    "r = result.join(movies,['mid']).select('title', 'rating','similarity')\n",
    "final_result= r.sort(r.similarity.desc()).limit(10)\n",
    "final_result\n",
    "\n",
    "#map_film = movies_without_MrSmith.rdd.flatMap(lambda x: ((mid_movieMrSmith,x.mid), similarity(mid_movieMrSmith, x.collect()[0][0])))\n",
    "#map_film = movies_without_MrSmith.rdd.flatMap(lambda x: ((mid_movieMrSmith,x.mid), lambda x: (len(actors.filter(actors.mid == x.mid).select('name').collect()[0][0]))))\n",
    "#movies_without_MrSmith.withColumn('rating', similarity(mid_movieMrSmith,movies_without_MrSmith.collect()[0][0])[0])\n",
    "#map_film.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4590.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4590.0 (TID 209429, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 275, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 454, in dumps\n    return pickle.dumps(obj, pickle_protocol)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/broadcast.py\", line 178, in __reduce__\n    raise Exception(\"Broadcast can only be serialized in driver\")\nException: Broadcast can only be serialized in driver\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 275, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 454, in dumps\n    return pickle.dumps(obj, pickle_protocol)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/broadcast.py\", line 178, in __reduce__\n    raise Exception(\"Broadcast can only be serialized in driver\")\nException: Broadcast can only be serialized in driver\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-d02a3d02e5e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mmap_film\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovies_without_MrSmith\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid_movieMrSmith\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmap_film\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m '''\n\u001b[1;32m     57\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_for_movies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4590.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4590.0 (TID 209429, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 275, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 454, in dumps\n    return pickle.dumps(obj, pickle_protocol)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/broadcast.py\", line 178, in __reduce__\n    raise Exception(\"Broadcast can only be serialized in driver\")\nException: Broadcast can only be serialized in driver\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 275, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 454, in dumps\n    return pickle.dumps(obj, pickle_protocol)\n  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/broadcast.py\", line 178, in __reduce__\n    raise Exception(\"Broadcast can only be serialized in driver\")\nException: Broadcast can only be serialized in driver\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "minYear = int(movies.select(min(movies.year)).collect()[0][0])\n",
    "maxYear = int(movies.select(max(movies.year)).collect()[0][0])\n",
    "maxRating = float(movies.filter(movies.rating != '\\\\N').select(max(movies.rating)).collect()[0][0])\n",
    "\n",
    "def get_mid(movie):\n",
    "    return movies.filter(movies.title == movie).select('mid').collect()[0][0]\n",
    "\n",
    "\n",
    "def similarity(mid_movie1, mid_movie2):\n",
    "    \n",
    "    get_num_actor = len(actors.filter(actors.mid == mid_movie1).select('name').collect())\n",
    "    get_num_tags = len(tags.filter(tags.mid == mid_movie1).select('tid').distinct().collect())\n",
    "    get_num_genres = len(genres.filter(genres.mid == mid_movie1).select('genre').distinct().collect())\n",
    "    \n",
    "    actorsMovie1 = actors.filter(actors.mid == mid_movie1).select('name')\n",
    "    actorsMovie2 = actors.filter(actors.mid == mid_movie2).select('name')\n",
    "    intersect_actor =  len(actorsMovie1.intersect(actorsMovie2).collect())/get_num_actor\n",
    "    \n",
    "    tagsMovie1 = tags.filter(tags.mid == mid_movie1).select('tid')\n",
    "    tagsMovie2 = tags.filter(tags.mid == mid_movie2).select('tid')\n",
    "    intersect_tag = len(tagsMovie1.intersect(tagsMovie2).collect())/get_num_tags\n",
    "    \n",
    "    genreMovie1 = genres.filter(genres.mid == mid_movie1).select('genre')\n",
    "    genreMovie2 = genres.filter(genres.mid == mid_movie2).select('genre')\n",
    "    intersect_genre =  len(genreMovie1.intersect(genreMovie2).collect())/get_num_genres\n",
    "    \n",
    "    year_movie1 = int(movies.filter(movies.mid == mid_movie1).select('year').collect()[0][0])\n",
    "    year_movie2 = int(movies.filter(movies.mid == mid_movie2).select('year').collect()[0][0])\n",
    "    difference = abs(year_movie1 - year_movie2)\n",
    "    age_gap =  1 - (difference/maxYear)\n",
    "    \n",
    "    \n",
    "    rating_movie1 = float(movies.filter(movies.mid == mid_movie1).select('rating').collect()[0][0])\n",
    "    rating_movie2 = float(movies.filter(movies.mid == mid_movie2).select('rating').collect()[0][0])\n",
    "    difference_gap = abs(rating_movie1 - rating_movie2) \n",
    "    rating_gap = 1 - (difference_gap/maxRating)\n",
    "    similarity = round((intersect_actor+intersect_tag+intersect_genre + age_gap + rating_gap)/5,2)*100\n",
    "    \n",
    "    myFloatRdd = sc.parallelize([similarity])\n",
    "    row = Row(\"similarity\") \n",
    "    result = myFloatRdd.map(row).toDF()\n",
    "\n",
    "    \n",
    "    return similarity#result.select('similarity')\n",
    "\n",
    "\n",
    "mid_movieMrSmith = get_mid('Mr. & Mrs. Smith')\n",
    "movies_without_MrSmith = movies.filter(movies.mid != mid_movieMrSmith).filter(movies.rating != '\\\\N').select('mid')\n",
    "similarity_for_movies = []\n",
    "\n",
    "#convertUDF = udf(lambda x,y: similarity(x,y),StringType())\n",
    "\n",
    "b = sc.broadcast(similarity)\n",
    "map_film = movies_without_MrSmith.rdd.flatMap(lambda x: ((mid_movieMrSmith,x.mid), b))\n",
    "map_film.take(5)\n",
    "'''\n",
    "rdd = sc.parallelize(similarity_for_movies)\n",
    "result = rdd.toDF().withColumnRenamed('_1','mid_movieMrSmith').withColumnRenamed('_2','mid').withColumnRenamed('_3','similarity')\n",
    "r = result.join(movies,['mid']).select('title', 'rating','similarity')\n",
    "final_result= r.sort(r.similarity.desc()).limit(10)\n",
    "final_result\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minYear = int(movies.select(min(movies.year)).collect()[0][0])\n",
    "maxYear = int(movies.select(max(movies.year)).collect()[0][0])\n",
    "maxRating = float(movies.filter(movies.rating != '\\\\N').select(max(movies.rating)).collect()[0][0])\n",
    "\n",
    "def get_mid(movie):\n",
    "    return movies.filter(movies.title == movie).select('mid').collect()[0][0]\n",
    "\n",
    "def get_num_actor(mid_film):\n",
    "    return len(actors.filter(actors.mid == mid_film).select('name').collect())\n",
    "\n",
    "def get_num_tags(mid_film):\n",
    "    return len(tags.filter(tags.mid == mid_film).select('tid').distinct().collect())\n",
    "\n",
    "def get_num_genres(mid_film):\n",
    "    return len(genres.filter(genres.mid == mid_film).select('genre').distinct().collect())\n",
    "\n",
    "def intersection_actors(mid_movie1, mid_movie2):\n",
    "    actorsMovie1 = actors.filter(actors.mid == mid_movie1).select('name')\n",
    "    actorsMovie2 = actors.filter(actors.mid == mid_movie2).select('name')\n",
    "    return len(actorsMovie1.intersect(actorsMovie2).collect())\n",
    "\n",
    "def intersection_tags(mid_movie1, mid_movie2):\n",
    "    tagsMovie1 = tags.filter(tags.mid == mid_movie1).select('tid')\n",
    "    tagsMovie2 = tags.filter(tags.mid == mid_movie2).select('tid')\n",
    "    return len(tagsMovie1.intersect(tagsMovie2).collect())\n",
    "\n",
    "def intersection_genres(mid_movie1, mid_movie2):\n",
    "    genreMovie1 = genres.filter(genres.mid == mid_movie1).select('genre')\n",
    "    genreMovie2 = genres.filter(genres.mid == mid_movie2).select('genre')\n",
    "    return len(genreMovie1.intersect(genreMovie2).collect())\n",
    "    \n",
    "def common_actors(movie1, movie2):\n",
    "    return intersection_actors(movie1, movie2)/get_num_actor(movie1)\n",
    "\n",
    "def common_tags(movie1, movie2):\n",
    "    return intersection_tags(movie1, movie2)/get_num_tags(movie1)\n",
    "\n",
    "def common_genres(movie1, movie2):\n",
    "    return intersection_genres(movie1, movie2)/get_num_genres(movie1)\n",
    "\n",
    "def age_gap(mid_movie1, mid_movie2):\n",
    "    year_movie1 = int(movies.filter(movies.mid == mid_movie1).select('year').collect()[0][0])\n",
    "    year_movie2 = int(movies.filter(movies.mid == mid_movie2).select('year').collect()[0][0])\n",
    "    difference = abs(year_movie1 - year_movie2) #check this site https://stats.stackexchange.com/questions/79706/normalizing-difference-between-two-real-values-to-0-1-interval \n",
    "    return 1 - (difference/maxYear)\n",
    "\n",
    "def rating_gap(mid_movie1, mid_movie2):\n",
    "    rating_movie1 = float(movies.filter(movies.mid == mid_movie1).select('rating').collect()[0][0])\n",
    "    rating_movie2 = float(movies.filter(movies.mid == mid_movie2).select('rating').collect()[0][0])\n",
    "    if rating_movie1 == '\\\\N' or rating_movie2 == '\\\\N':\n",
    "        difference = 0\n",
    "    else:\n",
    "        difference = abs(rating_movie1 - rating_movie2) #check this site https://stats.stackexchange.com/questions/79706/normalizing-difference-between-two-real-values-to-0-1-interval \n",
    "    return 1 - (difference/maxRating)\n",
    "\n",
    "def similarity_old(mid_movie1, mid_movie2):\n",
    "    return (common_actors(mid_movie1, mid_movie2)+common_tags(mid_movie1, mid_movie2)+common_genres(mid_movie1, mid_movie2) + age_gap(mid_movie1, mid_movie2) + rating_gap(mid_movie1, mid_movie2))/5\n",
    "\n",
    "\n",
    "#print('Similarity(Toy story, Toy story) =', float(\"{:.2f}\".format(similarity(get_mid('Toy story'),get_mid('Toy story')) *100)) , '%')\n",
    "#print('Similarity(Toy story, Mr. & Mrs. Smith) = ',float(\"{:.2f}\".format( similarity(get_mid('Toy story'),get_mid( 'Mr. & Mrs. Smith')) *100)) , '%')\n",
    "\n",
    "\n",
    "# start calculating top 10 similar movies \n",
    "# cross join of movies\n",
    "cross_join_movies = movies.withColumnRenamed('mid','mid1').crossJoin(movies.withColumnRenamed('mid','mid2')).select('mid1','mid2')\n",
    "#remove duplicates\n",
    "cross_join_without_duplicates = cross_join_movies.filter(cross_join_movies.mid1 != cross_join_movies.mid2)\n",
    "#map\n",
    "#map_movies = cross_join_without_duplicates.limit(10).rdd.map(lambda x: ((x.mid1, x.mid2),similarity(x.mid1,x.mid2)))\n",
    "#map_movies.take(5)\n",
    "\n",
    "#mid_movieMrSmith = get_mid('Mr. & Mrs. Smith')\n",
    "#movies_without_MrSmith = movies.filter(movies.title != 'Mr. & Mrs. Smith').collect()\n",
    "\n",
    "#for mid in movies_without_MrSmith[0][0]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mid(movie):\n",
    "    return movies.filter(movies.title == movie).select('mid').collect()[0][0]\n",
    "\n",
    "\n",
    "mid_actorsName = actors.sort('mid').select('mid','name')\n",
    "table_for_similarity = mid_actorsName.join(genres,['mid']).join(tags,['mid']).join(movies.select('mid','year','rating'),['mid'])\n",
    "\n",
    "mid_movieMrSmith = get_mid('Mr. & Mrs. Smith')\n",
    "table_sim = table_for_similarity.withColumn('midMr',lit(mid_movieMrSmith))\n",
    "\n",
    "actorsMRsmith = actors.filter(actors.mid == mid_movieMrSmith).withColumnRenamed('name','nameActMr').select('mid','nameActMr').withColumnRenamed('mid','midMr')\n",
    "genresMRsmith = genres.filter(genres.mid == mid_movieMrSmith).withColumnRenamed('genre','genreMr').select('mid','genreMr').withColumnRenamed('mid','midMr').join(actorsMRsmith,['midMr'])\n",
    "tagsMRsmith = tags.filter(tags.mid == mid_movieMrSmith).withColumnRenamed('tid','tidMr').select('mid','tidMr').withColumnRenamed('mid','midMr').join(genresMRsmith,['midMr'])\n",
    "yearMRsmith = movies.filter(movies.mid == mid_movieMrSmith).withColumnRenamed('year','yearMr').withColumnRenamed('rating', 'ratingMR').select('mid','yearMr','ratingMR').withColumnRenamed('mid','midMr').join(tagsMRsmith,['midMr'])\n",
    "\n",
    "final_tabel = yearMRsmith.join(table_sim,['midMr'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/serializers.py\", line 468, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 1097, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 357, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 437, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 789, in save_tuple\n",
      "    save(element)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 501, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 730, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 819, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 843, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 501, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 730, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 819, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 846, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 496, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 730, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 496, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\", line 730, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 890, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/luca/anaconda3/lib/python3.7/pickle.py\", line 524, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/home/luca/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/rdd.py\", line 279, in __getnewargs__\n",
      "    \"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\n",
      "Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlookedup_by_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlookedup_by_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kwdefaults'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwdefaults__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPPENDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlookedup_by_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlookedup_by_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kwdefaults'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwdefaults__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPPEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 or themodule is None):\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kwdefaults'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwdefaults__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 or themodule is None):\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kwdefaults'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwdefaults__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m         raise Exception(\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;34m\"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;34m\"action or transformation. RDD transformations and actions can only be invoked by the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-05890e6df222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#convertUDF = udf(lambda x,y: similarity(x,y),StringType())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmap_film\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovies_without_MrSmith\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid_movieMrSmith\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid_movieMrSmith\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmap_film\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0;32m-> 2629\u001b[0;31m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[1;32m   2630\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[1;32m   2631\u001b[0m                                              self.preservesPartitioning, self.is_barrier)\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2515\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2518\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2519\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2501\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2503\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2504\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/BigData-project/spark-3.0.2-bin-hadoop2.7/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "actorsRDD = actors.rdd\n",
    "\n",
    "\n",
    "def similarity(mid_movie1, mid_movie2):\n",
    "    get_num_actor = actorsRDD.filter(lambda x: x.mid == mid_movie2).map(lambda x: (x.mid,1)).reduceByKey(lambda a,b: a+b)\n",
    "    get_num_actor.take(10)\n",
    "    return get_num_actor.collect()[0][1]#result.select('similarity')\n",
    "\n",
    "mid_movieMrSmith = get_mid('Mr. & Mrs. Smith')\n",
    "movies_without_MrSmith = movies.filter(movies.mid != mid_movieMrSmith).filter(movies.rating != '\\\\N').select('mid')\n",
    "similarity_for_movies = []\n",
    "\n",
    "#convertUDF = udf(lambda x,y: similarity(x,y),StringType())\n",
    "map_film = movies_without_MrSmith.rdd.map(lambda x: ((mid_movieMrSmith,x.mid), similarity(mid_movieMrSmith,x.mid)))\n",
    "map_film.take(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
